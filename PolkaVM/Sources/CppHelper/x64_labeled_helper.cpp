// generated by polka.codes
// Label-based x86_64 JIT compilation for PolkaVM
// Single-pass compilation with lazy label creation for maximum performance

#include "helper.hh"
#include "jit_label_manager.hh"
#include "jit_control_flow.hh"
#include "jit_cfg_helper.hh"
#include "opcodes.hh"
#include <asmjit/x86.h>
#include <asmjit/core.h>
#include <algorithm>
#include <cstddef>
#include <cstdint>
#include <cstring>
#include <stdio.h>
#include <vector>
#include <unordered_map>
#include <mutex>
#include <memory>

using namespace asmjit;
using namespace asmjit::x86;
using namespace JIT;
using namespace PVM;

// Export function to create a new RuntimeContext (called from Swift)
extern "C" void* _Nonnull createRuntimeContext() noexcept {
    return new RuntimeContext();
}

// Export function to destroy a RuntimeContext (called from Swift)
extern "C" void destroyRuntimeContext(void* _Nonnull context) noexcept {
    delete static_cast<RuntimeContext*>(context);
}

// Export function to get dispatcher table for a function
extern "C" void* _Nullable * _Nullable getDispatcherTable(
    void* _Nonnull context,
    void* _Nonnull funcPtr,
    size_t* _Nonnull outSize) noexcept {
    auto* ctx = static_cast<RuntimeContext*>(context);
    auto& tables = ctx->dispatcherTables;
    auto it = tables.find(funcPtr);
    if (it != tables.end()) {
        *outSize = it->second.second;
        return it->second.first;
    }
    *outSize = 0;
    return nullptr;
}

// Export function to set dispatcher table for a function (called from Swift)
extern "C" void setDispatcherTable(
    void* _Nonnull context,
    void* _Nonnull funcPtr,
    void* _Nullable * _Nullable table,
    size_t size) noexcept {
    auto* ctx = static_cast<RuntimeContext*>(context);
    ctx->dispatcherTables[funcPtr] = {table, size};
}

// External declaration for the instruction emitter
extern "C" bool jit_emitter_emit_basic_block_instructions(
    void* _Nonnull assembler,
    const char* _Nonnull target_arch,
    const uint8_t* _Nonnull bytecode,
    uint32_t start_pc,
    uint32_t end_pc);

static inline uint32_t clamp_imm_len(uint32_t len) {
    return (len > 4) ? 4 : len;
}

static inline uint32_t second_immediate_len(uint32_t instr_size, uint32_t prefix_len, uint32_t first_immediate_len) {
    if (instr_size <= prefix_len + first_immediate_len) {
        return 0;
    }
    return clamp_imm_len(instr_size - prefix_len - first_immediate_len);
}

static inline uint64_t decode_immediate_raw(
    const uint8_t* bytecode,
    uint32_t offset,
    uint32_t len,
    uint32_t max_size)
{
    if (len == 0 || offset >= max_size || offset + len > max_size) {
        return 0;
    }
    uint64_t value = 0;
    for (uint32_t i = 0; i < len; ++i) {
        value |= uint64_t(bytecode[offset + i]) << (8 * i);
    }
    return value;
}

static inline int64_t decode_immediate_signed(
    const uint8_t* bytecode,
    uint32_t offset,
    uint32_t len,
    uint32_t max_size)
{
    if (len == 0) {
        return 0;
    }
    uint64_t value = decode_immediate_raw(bytecode, offset, len, max_size);
    if (len < 8) {
        const uint64_t sign_bit = uint64_t(1) << (len * 8 - 1);
        if (value & sign_bit) {
            value |= (~uint64_t(0)) << (len * 8);
        }
    }
    return static_cast<int64_t>(value);
}

static inline bool is_variable_length_opcode(uint8_t opcode) {
    switch (static_cast<Opcode>(opcode)) {
        case Opcode::Ecalli:
        case Opcode::StoreImmU8:
        case Opcode::StoreImmU16:
        case Opcode::StoreImmU32:
        case Opcode::StoreImmU64:
        case Opcode::LoadImm:
        case Opcode::LoadImmJump:
        case Opcode::LoadImmJumpInd:
        case Opcode::BranchEqImm:
        case Opcode::BranchNeImm:
        case Opcode::BranchLtUImm:
        case Opcode::BranchLeUImm:
        case Opcode::BranchGeUImm:
        case Opcode::BranchGtUImm:
        case Opcode::BranchLtSImm:
        case Opcode::BranchLeSImm:
        case Opcode::BranchGeSImm:
        case Opcode::BranchGtSImm:
        case Opcode::StoreImmIndU8:
        case Opcode::StoreImmIndU16:
        case Opcode::StoreImmIndU32:
        case Opcode::StoreImmIndU64:
        case Opcode::AddImm32:
        case Opcode::AndImm:
        case Opcode::XorImm:
        case Opcode::OrImm:
        case Opcode::MulImm32:
        case Opcode::SetLtUImm:
        case Opcode::SetLtSImm:
        case Opcode::ShloLImm32:
        case Opcode::ShloRImm32:
        case Opcode::SharRImm32:
        case Opcode::NegAddImm32:
        case Opcode::SetGtUImm:
        case Opcode::SetGtSImm:
        case Opcode::ShloLImmAlt32:
        case Opcode::ShloRImmAlt32:
        case Opcode::SharRImmAlt32:
        case Opcode::CmovIzImm:
        case Opcode::CmovNzImm:
            return true;
        default:
            return false;
    }
}

// Helper to get instruction size
static uint32_t getInstructionSize(const uint8_t* bytecode, uint32_t pc, size_t bytecode_size) {
    uint8_t opcode = bytecode[pc];

    // Handle old opcode numbers for backwards compatibility
    // Old JumpInd = opcode 2 (2 bytes: [opcode][reg_index])
    if (opcode == 2) {
        return 2;
    }

    return get_instruction_size(bytecode, pc, bytecode_size);
}

// Helper to extract jump target from instruction
// Returns the target PC for branch instructions, or fallthrough PC for non-branches
static uint32_t getJumpTarget(const uint8_t* bytecode, uint32_t pc, uint32_t instrSize, size_t bytecodeSize) {
    uint8_t opcode = bytecode[pc];

    // Jump: [opcode][offset_32bit] = 5 bytes
    if (opcode_is(opcode, Opcode::Jump)) {
        if (pc + 5 > bytecodeSize) {
            fprintf(stderr, "[JIT] Jump instruction truncated at PC %u (need 5 bytes, have %zu)\n", pc, bytecodeSize);
            return pc + instrSize; // Fallthrough on error
        }
        uint32_t offset;
        memcpy(&offset, &bytecode[pc + 1], 4);
        return pc + int32_t(offset);
    }

    // Branch register instructions (fixed 7 bytes): [opcode][reg1][reg2][offset_32bit]
    if (opcode_is(opcode, Opcode::BranchEq) ||
        opcode_is(opcode, Opcode::BranchNe) ||
        opcode_is(opcode, Opcode::BranchLtU) ||
        opcode_is(opcode, Opcode::BranchLtS) ||
        opcode_is(opcode, Opcode::BranchGeU) ||
        opcode_is(opcode, Opcode::BranchGeS))
    {
        if (instrSize < 7 || pc + 7 > bytecodeSize) {
            fprintf(stderr, "[JIT] Branch instruction truncated at PC %u (need 7 bytes, have %zu)\n", pc, bytecodeSize);
            return pc + instrSize;
        }
        uint32_t offset = 0;
        memcpy(&offset, &bytecode[pc + 3], 4);
        return pc + int32_t(offset);
    }

    // Branch immediate instructions:
    // [opcode][packed_rA_lX][immed_X][immed_Y]
    if (opcode_is(opcode, Opcode::BranchEqImm) ||
        opcode_is(opcode, Opcode::BranchNeImm) ||
        opcode_is(opcode, Opcode::BranchLtUImm) ||
        opcode_is(opcode, Opcode::BranchLeUImm) ||
        opcode_is(opcode, Opcode::BranchGeUImm) ||
        opcode_is(opcode, Opcode::BranchGtUImm) ||
        opcode_is(opcode, Opcode::BranchLtSImm) ||
        opcode_is(opcode, Opcode::BranchLeSImm) ||
        opcode_is(opcode, Opcode::BranchGeSImm) ||
        opcode_is(opcode, Opcode::BranchGtSImm))
    {
        if (instrSize < 2 || pc + instrSize > bytecodeSize) {
            return pc + instrSize;
        }
        uint8_t packed = bytecode[pc + 1];
        uint32_t l_X = clamp_imm_len((packed >> 4) & 0x07);
        if (instrSize < (2 + l_X)) {
            return pc + instrSize;
        }
        const uint32_t l_Y = second_immediate_len(instrSize, 2, l_X);
        int32_t offset = static_cast<int32_t>(decode_immediate_signed(bytecode, pc + 2 + l_X, l_Y, static_cast<uint32_t>(bytecodeSize)));
        return pc + uint32_t(offset);
    }

    // LoadImmJump: [opcode][r_A | l_X][immed_X (l_X bytes)][immed_Y (l_Y bytes)]
    // CRITICAL: Use actual instruction size (from skip table) to calculate l_Y
    // Format: [opcode][r_A|l_X][immed_X (l_X bytes)][immed_Y (l_Y bytes)]
    // Size = 2 + l_X + l_Y, therefore: l_Y = instrSize - 2 - l_X
    if (opcode_is(opcode, Opcode::LoadImmJump)) {
        if (pc + 1 >= bytecodeSize) return pc + instrSize;

        uint8_t byte1 = bytecode[pc + 1];
        uint32_t l_X = (byte1 >> 4) & 0x07;
        if (l_X > 4) l_X = 4;

        if (instrSize < (2 + l_X)) {
            // Invalid instruction, return fallthrough
            return pc + instrSize;
        }
        const uint32_t l_Y = second_immediate_len(instrSize, 2, l_X);

        // Format: [opcode][byte1][immed_X (l_X bytes)][immed_Y (l_Y bytes)]
        // immed_Y (jump offset) starts at: pc + 2 + l_X
        uint32_t offsetPos = pc + 2 + l_X;

        // Bounds check for offset read
        if (offsetPos + l_Y > bytecodeSize) {
            return pc + instrSize;
        }

        int32_t jumpOffset = static_cast<int32_t>(decode_immediate_signed(bytecode, offsetPos, l_Y, static_cast<uint32_t>(bytecodeSize)));
        return pc + uint32_t(jumpOffset);
    }

    return pc + instrSize; // Fallthrough
}

// Main compilation function with labels
extern "C" int32_t compilePolkaVMCode_x64_labeled(
    void* _Nonnull context,
    const uint8_t* _Nonnull codeBuffer,
    size_t codeSize,
    uint32_t initialPC,
    uint32_t jitMemorySize,
    const uint32_t* _Nullable skipTable,
    size_t skipTableSize,
    const uint8_t* _Nullable bitmask,
    size_t bitmaskSize,
    void* _Nullable * _Nonnull funcOut)
{
    auto* ctx = static_cast<RuntimeContext*>(context);

    // Validate inputs
    if (!codeBuffer || codeSize == 0) {
        return 1; // Invalid input
    }

    if (!funcOut) {
        return 2; // Invalid output parameter
    }

    // Helper lambda to check if a PC is at an instruction boundary using bitmask
    // Per spec pvm.tex, instruction boundaries have bitmask bit 0 set
    auto isInstructionBoundary = [&](uint32_t pc) -> bool {
        if (!bitmask || pc >= codeSize) {
            return false;
        }
        uint32_t byteIndex = pc / 8;
        uint32_t bitIndex = pc % 8;
        if (byteIndex >= bitmaskSize) {
            return false;
        }
        return (bitmask[byteIndex] & (1 << bitIndex)) != 0;
    };

    // Create lambda to get instruction size using skip table.
    // Variable-length opcodes must use skip/bitmask rather than local re-decoding.
    auto getInstrSize = [&](uint32_t pc) -> uint32_t {
        if (pc >= codeSize) {
            return 0;
        }
        uint8_t opcode = codeBuffer[pc];
        const bool variableOpcode = is_variable_length_opcode(opcode);

        if (variableOpcode && skipTable != nullptr && pc < skipTableSize) {
            uint32_t skip = skipTable[pc];
            if (pc + skip + 1 > codeSize) {
                uint32_t maxSkip = codeSize - pc - 1;
                fprintf(stderr, "[JIT] Invalid skip value %u at PC %u (max: %u)\n", skip, pc, maxSkip);
                return 0;
            }
            return skip + 1;
        }

        if (variableOpcode && bitmask != nullptr) {
            for (uint32_t nextPC = pc + 1; nextPC < codeSize; ++nextPC) {
                if (bitmask[nextPC / 8] & (1 << (nextPC % 8))) {
                    return nextPC - pc;
                }
            }
            return codeSize - pc;
        }

        uint32_t fixedSize = getInstructionSize(codeBuffer, pc, codeSize);
        if (fixedSize > 0 && fixedSize <= 16) {
            return fixedSize;
        }

        // Fallback: if caller supplied skip table but opcode wasn't classified above,
        // use it as a last resort for robustness.
        if (skipTable != nullptr && pc < skipTableSize) {
            uint32_t skip = skipTable[pc];
            if (pc + skip + 1 > codeSize) {
                uint32_t maxSkip = codeSize - pc - 1;
                fprintf(stderr, "[JIT] Invalid skip value %u at PC %u (max: %u), using fallback\n", skip, pc, maxSkip);
                return fixedSize;
            }
            return skip + 1;
        }

        return fixedSize;
    };

    // Use the runtime from the context (owned by Swift)
    CodeHolder code;
    Error err = code.init(ctx->runtime->environment());
    if (err != kErrorOk) {
        fprintf(stderr, "[JIT] Failed to initialize CodeHolder: %d\n", err);
        return int32_t(err);
    }

    // Validate runtime state before creating assembler
    if (!ctx->runtime) {
        fprintf(stderr, "[JIT] Runtime context is null\n");
        return 5; // Compilation error
    }

    // Create x86 assembler
    x86::Assembler a(&code);

    // Prologue: save callee-saved registers
    a.push(rbx);
    a.push(rbp);
    a.push(r12);
    a.push(r13);
    a.push(r14);
    a.push(r15);

    // Setup VM environment registers
    a.mov(rbx, rdi);  // registers_ptr -> rbx
    a.mov(r12, rsi);  // memory_base_ptr -> r12
    a.mov(x86::r13, x86::edx);  // memory_size -> r13 (zero-extends)
    a.mov(r14, rcx);  // gas_ptr -> r14
    a.mov(r15d, r8d); // initial_pvm_pc -> r15d (PC)
    a.mov(rbp, r9);   // invocation_context_ptr -> rbp

    // Create label manager
    LabelManager labelManager;

    // We'll build the dispatcher table after compilation, using labels that get created
    // during the normal compilation process (via labelManager)

    // Create exit labels for different exit conditions
    Label exitLabel = a.new_label();       // Normal exit (halt) - eax = 0
    Label panicLabel = a.new_label();      // Panic exit (trap, address < 65536) - eax = -1
    Label pagefaultLabel = a.new_label();  // Page fault exit (address >= memory_size) - eax = 3
    Label outOfGasLabel = a.new_label();   // Out of gas exit - eax = 1
    Label unsupportedLabel = a.new_label();// Unsupported instruction - exit to interpreter - eax = 2
    Label dispatcherLoop = a.new_label();  // Dispatcher loop for indirect jumps without jump table
    Label epilogueLabel = a.new_label();   // Epilogue (restore registers and return)
    Label dispatcherLoopNoJumpTable = a.new_label();  // For JumpInd without jump table
    Label dispatcherLoopNoJumpTable2 = a.new_label(); // For LoadImmJumpInd without jump table

    // Pass the PC label map to labelManager for use during compilation
    // We'll store it separately and use it to build the dispatcher jump table

    // === PRE-PASS: Identify all jump targets ===
    // This is necessary to handle backward jumps (loops)
    // Also check if we need dispatcher table (has JumpInd without jump table support)
    bool needsDispatcherTable = false;
    uint32_t pc = 0;
    while (pc < codeSize) {
        // CRITICAL: Only process instruction boundaries
        // Skip PCs that are not at instruction boundaries (data, padding, etc.)
        if (bitmask && !isInstructionBoundary(pc)) {
            pc++;
            continue;
        }

        uint8_t opcode = codeBuffer[pc];
        uint32_t instrSize = getInstrSize(pc);  // Now handles fixed-size correctly

        if (instrSize == 0) {
            // Unknown opcode - skip and treat as data
            // This can happen when the bitmask marks data as an instruction boundary
            fprintf(stderr, "[JIT] Unknown opcode %d (0x%02X) at PC %u, treating as data\n", opcode, opcode, pc);
            pc++;
            continue;
        }

        // Check if this code has JumpInd, LoadImmJump, or LoadImmJumpInd (which need dispatcher)
        if (opcode_is(opcode, Opcode::JumpInd) ||
            opcode_is(opcode, Opcode::LoadImmJump) ||
            opcode_is(opcode, Opcode::LoadImmJumpInd)) {
            needsDispatcherTable = true;
        }

        // Mark jump targets for all control flow instructions
        // This is necessary to handle backward jumps (loops) correctly
        // IMPORTANT: Only validate known jump/branch opcodes - don't use size-based heuristics
        // because size-based detection can misidentify data as code
        // Note: LoadImmJumpInd is a runtime jump (target depends on register value),
        // so we cannot validate its target at compile time
        bool isJumpInstruction = opcode_is(opcode, Opcode::Jump) ||
                                  opcode_is(opcode, Opcode::LoadImmJump);

        bool isBranchInstruction = opcode_is(opcode, Opcode::BranchEq) ||
                                   opcode_is(opcode, Opcode::BranchNe) ||
                                   opcode_is(opcode, Opcode::BranchLtU) ||
                                   opcode_is(opcode, Opcode::BranchLtS) ||
                                   opcode_is(opcode, Opcode::BranchGeU) ||
                                   opcode_is(opcode, Opcode::BranchGeS) ||
                                   opcode_is(opcode, Opcode::BranchEqImm) ||
                                   opcode_is(opcode, Opcode::BranchNeImm) ||
                                   opcode_is(opcode, Opcode::BranchLtUImm) ||
                                   opcode_is(opcode, Opcode::BranchLeUImm) ||
                                   opcode_is(opcode, Opcode::BranchGeUImm) ||
                                   opcode_is(opcode, Opcode::BranchGtUImm) ||
                                   opcode_is(opcode, Opcode::BranchLtSImm) ||
                                   opcode_is(opcode, Opcode::BranchLeSImm) ||
                                   opcode_is(opcode, Opcode::BranchGeSImm) ||
                                   opcode_is(opcode, Opcode::BranchGtSImm);

        bool isRuntimeJump = opcode_is(opcode, Opcode::LoadImmJumpInd);

        if (isJumpInstruction || isBranchInstruction) {
            uint32_t targetPC = getJumpTarget(codeBuffer, pc, instrSize, codeSize);
            if (targetPC != pc + instrSize) {
                // This is a branch/jump instruction (not a fallthrough)
                // VALIDATE: Check if target is within code bounds and at an instruction boundary
                if (targetPC >= codeSize) {
                    // Invalid jump target - skip this instruction and continue
                    // Don't log spam - this is expected for blobs with data
                    pc += instrSize;
                    continue;
                }
                if (bitmask && !isInstructionBoundary(targetPC)) {
                    // Invalid jump target - skip this instruction and continue
                    pc += instrSize;
                    continue;
                }
                labelManager.markJumpTarget(targetPC);
            }
        } else if (isRuntimeJump) {
            // LoadImmJumpInd jumps through a register - cannot validate target at compile time
            // Skip validation and label marking for runtime jumps
        }

        // Advance to next instruction
        pc += instrSize;
    }

    // === BUILD CONTROL FLOW GRAPH ===
    // CFG analysis ensures we only compile code reachable from entry point (PC=0)
    // This matches interpreter behavior and reduces semantic differences
    ControlFlowGraph cfg;
    cfg.build(codeBuffer, codeSize, skipTable, skipTableSize, bitmask, /* entryPC= */ 0);

    fprintf(stderr, "[JIT CFG] Found %zu reachable PCs out of %zu total PCs\n",
            cfg.getReachablePCs().size(), static_cast<size_t>(codeSize));

    // Transfer CFG jump targets to label manager
    // The CFG correctly identifies jump targets using bitmask scanning for variable-length instructions
    // We need to inform the label manager about these targets so jump instructions can find them
    for (uint32_t pc = 0; pc < codeSize; pc++) {
        if (cfg.isJumpTarget(pc)) {
            labelManager.markJumpTarget(pc);
        }
    }

    // === GAS ACCOUNTING HELPER ===
    // Lambda to deduct gas and check for exhaustion
    // Returns true if gas was deducted successfully, false if out of gas
    auto deductGas = [&](uint64_t gasCost) -> void {
        // Load current gas value from r14 (VM_GAS_PTR)
        a.mov(x86::rax, x86::qword_ptr(x86::r14));
        // Subtract gas cost (sets CF if borrow/underflow occurred)
        a.sub(x86::rax, gasCost);
        // Store updated value back (preserves flags)
        a.mov(x86::qword_ptr(x86::r14), x86::rax);
        // Jump if borrow occurred (unsigned underflow)
        a.jb(outOfGasLabel);
    };

    // === MAIN COMPILATION PASS: ONLY COMPILE REACHABLE CODE ===
    // Iterate through PCs in sequential order to ensure fallthrough works correctly
    // Only compile PCs that are marked as reachable by the CFG
    const auto& reachablePCs = cfg.getReachablePCs();

    pc = 0;  // Reset PC for main compilation pass
    uint8_t opcode;
    uint32_t instrSize;
    bool instructionHandled;

    while (pc < codeSize) {
        // Skip PCs that are not reachable
        if (!reachablePCs.contains(pc)) {
            pc++;
            goto next_instruction;
        }

        opcode = codeBuffer[pc];
        instrSize = getInstrSize(pc);

        fprintf(stderr, "[JIT] Compiling PC %u (opcode=0x%02x, instrSize=%u)\n", pc, opcode, instrSize);

        if (instrSize == 0) {
            // Unknown opcode - skip even in reachable code
            // This shouldn't happen in valid reachable code, but handle gracefully
            fprintf(stderr, "[JIT] Unknown opcode %d (0x%02X) at reachable PC %u, skipping\n", opcode, opcode, pc);
            pc++;
            goto next_instruction;
        }

        // Bind label for this PC if:
        // 1. It's a jump target (for branches/jumps)
        // 2. It was marked in pre-pass
        // 3. OR we need a dispatcher table (for JumpInd to work)
        if (labelManager.isMarkedTarget(pc) || labelManager.isJumpTarget(pc) || cfg.isJumpTarget(pc)) {
            labelManager.bindLabel(&a, pc, "x86_64");
        } else if (needsDispatcherTable) {
            // For dispatcher table, we need labels for ALL reachable instruction PCs
            // This allows JumpInd to jump to any reachable PC
            labelManager.bindLabel(&a, pc, "x86_64");
        }

        // === GAS ACCOUNTING ===
        // Deduct 1 gas for this instruction (default gas cost per PVM spec)
        // Check AFTER binding labels so jump targets don't double-deduct
        deductGas(1);

        // Variable to track if we handled this instruction (for PC advancement)
        instructionHandled = false;

        // Handle control flow instructions with labels
        if (opcode_is(opcode, Opcode::Jump)) {
            uint32_t targetPC = getJumpTarget(codeBuffer, pc, instrSize, codeSize);

            // CRITICAL: Only compile jump if target was validated in pre-pass
            // If target is beyond code size or not a jump target, fall through
            if (targetPC >= codeSize || !labelManager.isJumpTarget(targetPC)) {
                // Invalid jump target - treat as fallthrough (don't jump)
                goto next_instruction;
            }

            Label targetLabel = labelManager.getOrCreateLabel(&a, targetPC, "x86_64");
            jit_emit_jump_labeled(&a, "x86_64", targetLabel);
            goto next_instruction;
        }

        if (opcode_is(opcode, Opcode::BranchEq)) {
            uint8_t reg1 = codeBuffer[pc + 1];
            uint8_t reg2 = codeBuffer[pc + 2];
            uint32_t targetPC = getJumpTarget(codeBuffer, pc, instrSize, codeSize);

            // CRITICAL: Only compile branch if target was validated in pre-pass
            if (targetPC >= codeSize || !labelManager.isJumpTarget(targetPC)) {
                // Invalid jump target - treat as fallthrough
                goto next_instruction;
            }

            Label targetLabel = labelManager.getOrCreateLabel(&a, targetPC, "x86_64");
            jit_emit_branch_eq_labeled(&a, "x86_64", reg1, reg2, targetLabel);

            goto next_instruction;
        }

        if (opcode_is(opcode, Opcode::BranchNe)) {
            uint8_t reg1 = codeBuffer[pc + 1];
            uint8_t reg2 = codeBuffer[pc + 2];
            uint32_t targetPC = getJumpTarget(codeBuffer, pc, instrSize, codeSize);

            // CRITICAL: Only compile branch if target was validated in pre-pass
            if (targetPC >= codeSize || !labelManager.isJumpTarget(targetPC)) {
                // Invalid jump target - treat as fallthrough
                goto next_instruction;
            }

            Label targetLabel = labelManager.getOrCreateLabel(&a, targetPC, "x86_64");
            jit_emit_branch_ne_labeled(&a, "x86_64", reg1, reg2, targetLabel);

            goto next_instruction;
        }

        if (opcode_is(opcode, Opcode::LoadImmJump)) {
            fprintf(stderr, "[JIT] LoadImmJump at PC %u: opcode=0x%02x, byte1=0x%02x\n", pc, opcode, codeBuffer[pc + 1]);

            // Per spec pvm.tex section 5.10: [opcode][r_A | l_X][immed_X (l_X bytes)][immed_Y (l_Y bytes)]
            uint8_t byte1 = codeBuffer[pc + 1];
            uint8_t destReg = byte1 & 0x0F;  // r_A (lower 4 bits)
            uint32_t l_X = (byte1 >> 4) & 0x07;  // Length of immed_X (bits 4-6)

            fprintf(stderr, "[JIT] LoadImmJump: destReg=%u, l_X=%u, instrSize=%u\n", destReg, l_X, instrSize);

            // Safety: limit l_X to reasonable range
            if (l_X > 4) l_X = 4;

            // CRITICAL: Calculate l_Y from actual instruction size (from skip table)
            // Format: [opcode][r_A|l_X][immed_X (l_X bytes)][immed_Y (l_Y bytes)]
            // Size = 2 + l_X + l_Y, therefore: l_Y = instrSize - 2 - l_X
            uint32_t l_Y = instrSize - 2 - l_X;

            // Validate l_Y is in valid range (0-4 per spec)
            if (l_Y > 4 || (instrSize < 2 + l_X)) {
                fprintf(stderr, "[JIT] Invalid LoadImmJump at PC %u: l_X=%u, l_Y=%u, instrSize=%u (skiptable=%u)\n",
                        pc, l_X, l_Y, instrSize, skipTable ? skipTable[pc] : 0);
                return 3; // Compilation error
            }

            // Decode immed_X (l_X bytes, little-endian)
            uint64_t immediate = 0;
            for (uint32_t i = 0; i < l_X; i++) {
                if (pc + 2 + i >= codeSize) return 3;  // Bounds check
                immediate |= (uint64_t)codeBuffer[pc + 2 + i] << (8 * i);
            }
            // Sign-extend if needed (for signed values)
            if (l_X > 0 && l_X < 8) {
                uint64_t signBit = 1ULL << (l_X * 8 - 1);
                if (immediate & signBit) {
                    // Sign-extend to 64-bit
                    immediate |= (~0ULL) << (l_X * 8);
                }
            }

            // Decode immed_Y (jump offset, l_Y bytes)
            uint64_t jumpOffset = 0;
            for (uint32_t i = 0; i < l_Y; i++) {
                if (pc + 2 + l_X + i >= codeSize) return 3;  // Bounds check
                jumpOffset |= (uint64_t)codeBuffer[pc + 2 + l_X + i] << (8 * i);
            }

            fprintf(stderr, "[JIT] LoadImmJump: jumpOffset=%lu (0x%lx), l_Y=%u\n", (unsigned long)jumpOffset, (unsigned long)jumpOffset, l_Y);

            // Sign-extend offset
            if (l_Y > 0 && l_Y < 8) {
                uint64_t signBit = 1ULL << (l_Y * 8 - 1);
                if (jumpOffset & signBit) {
                    jumpOffset |= (~0ULL) << (l_Y * 8);
                }
            }

            uint32_t targetPC = pc + uint32_t(int32_t(jumpOffset));  // Offset is relative

            // CRITICAL: Only compile jump if target is within bounds and was validated
            if (targetPC >= codeSize || !labelManager.isJumpTarget(targetPC)) {
                // Invalid jump target - treat as fallthrough (don't jump)
                fprintf(stderr, "[JIT] LoadImmJump at PC %u: invalid target %u, treating as fallthrough\n", pc, targetPC);
                goto next_instruction;
            }

            Label targetLabel = labelManager.getOrCreateLabel(&a, targetPC, "x86_64");
            jit_emit_load_imm_jump_labeled(&a, "x86_64", destReg, immediate, targetLabel);

            goto next_instruction;
        }

        if (opcode_is(opcode, Opcode::JumpInd)) {
            // JumpInd: [opcode][reg_index] - jump to address stored in register
            uint8_t ptr_reg = codeBuffer[pc + 1];

            // Load target address from register
            a.mov(x86::eax, x86::dword_ptr(x86::rbx, ptr_reg * 8));

            // Special case: check for halt address (0xFFFF0000)
            // This is the djumpHaltAddress constant from Instructions+Helpers.swift
            a.cmp(x86::eax, 0xFFFF0000);
            a.je(exitLabel);  // Jump to halt exit

            // Check if jump table is available
            a.mov(x86::rdx, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, jumpTableData)));
            a.test(x86::rdx, x86::rdx);
            a.jz(dispatcherLoopNoJumpTable);  // No jump table, update PC and use dispatcher loop instead

            // Load jump table parameters
            a.mov(x86::ecx, x86::dword_ptr(x86::rbp, offsetof(JITHostFunctionTable, alignmentFactor)));
            a.mov(x86::r8d, x86::dword_ptr(x86::rbp, offsetof(JITHostFunctionTable, jumpTableEntrySize)));
            a.mov(x86::r9d, x86::dword_ptr(x86::rbp, offsetof(JITHostFunctionTable, jumpTableEntriesCount)));

            // Validate target != 0
            a.test(x86::eax, x86::eax);
            a.jz(panicLabel);  // Invalid target (0) - changed from pagefaultLabel

            // Validate target alignment
            a.mov(x86::r10d, x86::eax);  // Copy target
            a.test(x86::ecx, x86::ecx);  // Check if alignmentFactor is 0
            a.jz(panicLabel);  // Division by zero is invalid - changed from pagefaultLabel
            a.mov(x86::edx, 0);   // Clear edx for division
            a.div(x86::ecx);  // Divide by alignmentFactor, quotient in eax, remainder in edx
            a.cmp(x86::edx, 0);  // Check if remainder is 0 (aligned)
            a.jne(panicLabel);  // Not aligned, invalid jump - changed from pagefaultLabel

            // Calculate entry index: (target / alignmentFactor) - 1
            a.dec(x86::eax);  // Decrement to get entry index
            a.cmp(x86::eax, x86::r9d);  // Compare with entriesCount
            a.jae(panicLabel);  // Index >= entriesCount, invalid jump - changed from pagefaultLabel

            // Calculate byte offset: entryIndex * entrySize
            a.mov(x86::edx, x86::eax);  // Copy entry index
            a.imul(x86::edx, x86::r8d);  // Multiply by entry size

            // Load jump table data pointer
            a.mov(x86::r11, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, jumpTableData)));

            // Read entry based on entrySize
            // For now, support entry sizes 1, 2, 3, 4
            Label entrySize1 = a.new_label();
            Label entrySize2 = a.new_label();
            Label entrySize3 = a.new_label();
            Label entrySize4 = a.new_label();
            Label readDone = a.new_label();

            a.cmp(x86::r8d, 1);
            a.je(entrySize1);
            a.cmp(x86::r8d, 2);
            a.je(entrySize2);
            a.cmp(x86::r8d, 3);
            a.je(entrySize3);
            a.cmp(x86::r8d, 4);
            a.je(entrySize4);
            // Invalid entry size, fall back to interpreter
            a.jmp(unsupportedLabel);

            // Entry size 1: Read UInt8
            a.bind(entrySize1);
            a.movzx(x86::eax, x86::byte_ptr(x86::r11, x86::rdx));
            a.jmp(readDone);

            // Entry size 2: Read UInt16
            a.bind(entrySize2);
            a.movzx(x86::eax, x86::word_ptr(x86::r11, x86::rdx));
            a.jmp(readDone);

            // Entry size 3: Read UInt24
            a.bind(entrySize3);
            a.movzx(x86::eax, x86::byte_ptr(x86::r11, x86::rdx));  // Read byte 0
            a.mov(x86::r10d, x86::eax);  // Save byte 0
            // Read bytes 1-2: [r11 + rdx + 1] using base + index + displacement
            // Format: [base + index*scale + displacement]
            a.movzx(x86::eax, x86::word_ptr(x86::r11, x86::rdx, 0, 1));  // Read bytes 1-2
            a.shl(x86::eax, 8);  // Shift to make room for byte 0
            a.or_(x86::eax, x86::r10d);  // Combine byte 0
            a.jmp(readDone);

            // Entry size 4: Read UInt32
            a.bind(entrySize4);
            a.mov(x86::eax, x86::dword_ptr(x86::r11, x86::rdx));
            a.jmp(readDone);

            a.bind(readDone);

            // Update PC with the looked-up address
            a.mov(x86::r15d, x86::eax);

            // Jump to the new PC
            // First, check if the new PC is within valid range
            a.cmp(x86::eax, codeSize);
            a.jae(pagefaultLabel);  // PC out of bounds

            // For now, fall back to interpreter with the updated PC
            // This is safe because the jump table lookup validated the target
            a.mov(x86::eax, 2);  // Exit code 2 = exit to interpreter with updated PC
            a.jmp(epilogueLabel);

            // No jump table available: update PC and fall back to dispatcher
            a.bind(dispatcherLoopNoJumpTable);
            a.mov(x86::r15d, x86::eax);  // Update PC with target address
            a.jmp(dispatcherLoop);
        }

        if (opcode_is(opcode, Opcode::LoadImmJumpInd)) {
            // LoadImmJumpInd: [opcode][packed_ra_rb][len][immed_X][immed_Y]
            if (instrSize < 3 || pc + instrSize > codeSize) {
                fprintf(stderr, "[JIT] LoadImmJumpInd instruction truncated at PC %u (size=%u, codeSize=%zu)\n", pc, instrSize, codeSize);
                return 3; // Compilation error
            }

            uint8_t packed_regs = codeBuffer[pc + 1];
            uint8_t dest_reg = packed_regs & 0x0F;  // bits 0-3
            uint8_t base_reg = (packed_regs >> 4) & 0x0F;  // bits 4-7

            uint8_t len_byte = codeBuffer[pc + 2];
            uint32_t l_X = clamp_imm_len(len_byte & 0x07);
            if (instrSize < (3 + l_X)) {
                fprintf(stderr, "[JIT] LoadImmJumpInd invalid lengths at PC %u (l_X=%u, instrSize=%u)\n", pc, l_X, instrSize);
                return 3;
            }
            uint32_t l_Y = second_immediate_len(instrSize, 3, l_X);

            int64_t value = decode_immediate_signed(codeBuffer, pc + 3, l_X, static_cast<uint32_t>(codeSize));
            int64_t offset = decode_immediate_signed(codeBuffer, pc + 3 + l_X, l_Y, static_cast<uint32_t>(codeSize));

            // Load immediate into destination register.
            a.mov(x86::rax, static_cast<uint64_t>(value));
            a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rax);

            // Load target address from base register
            // IMPORTANT: Zero-extend by loading into rax directly, not eax
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, base_reg * 8));

            // Add offset
            a.mov(x86::rcx, static_cast<uint64_t>(offset));
            a.add(x86::rax, x86::rcx);

            // Special case: check for halt address (0xFFFF0000)
            // This is the djumpHaltAddress constant from Instructions+Helpers.swift
            // IMPORTANT: Must compare full 64-bit value to match halt address
            a.mov(x86::rdx, 0xFFFF0000ull);  // Load 64-bit immediate
            a.cmp(x86::rax, x86::rdx);  // Compare 64-bit registers
            a.je(exitLabel);

            // Check if jump table is available
            a.mov(x86::rdx, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, jumpTableData)));
            a.test(x86::rdx, x86::rdx);
            a.jz(dispatcherLoopNoJumpTable2);  // No jump table, update PC and use dispatcher loop instead

            // Load jump table parameters
            a.mov(x86::ecx, x86::dword_ptr(x86::rbp, offsetof(JITHostFunctionTable, alignmentFactor)));
            a.mov(x86::r8d, x86::dword_ptr(x86::rbp, offsetof(JITHostFunctionTable, jumpTableEntrySize)));
            a.mov(x86::r9d, x86::dword_ptr(x86::rbp, offsetof(JITHostFunctionTable, jumpTableEntriesCount)));

            // Special case: if jump table is empty (0 entries), use dispatcher loop instead
            a.test(x86::r9d, x86::r9d);
            a.jz(dispatcherLoopNoJumpTable2);

            // Validate target != 0
            a.test(x86::rax, x86::rax);
            a.jz(pagefaultLabel);

            // Validate target alignment
            a.mov(x86::r10d, x86::eax);
            a.test(x86::ecx, x86::ecx);
            a.jz(pagefaultLabel);
            a.mov(x86::edx, 0);
            a.div(x86::ecx);
            a.cmp(x86::edx, 0);
            a.jne(pagefaultLabel);

            // Calculate entry index: (target / alignmentFactor) - 1
            a.dec(x86::eax);
            a.cmp(x86::eax, x86::r9d);
            a.jae(pagefaultLabel);

            // Calculate byte offset: entryIndex * entrySize
            a.mov(x86::edx, x86::eax);
            a.imul(x86::edx, x86::r8d);

            // Load jump table data pointer
            a.mov(x86::r11, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, jumpTableData)));

            // Read entry based on entrySize
            Label entrySize1 = a.new_label();
            Label entrySize2 = a.new_label();
            Label entrySize3 = a.new_label();
            Label entrySize4 = a.new_label();
            Label readDone = a.new_label();

            a.cmp(x86::r8d, 1);
            a.je(entrySize1);
            a.cmp(x86::r8d, 2);
            a.je(entrySize2);
            a.cmp(x86::r8d, 3);
            a.je(entrySize3);
            a.cmp(x86::r8d, 4);
            a.je(entrySize4);
            a.jmp(unsupportedLabel);

            a.bind(entrySize1);
            a.movzx(x86::eax, x86::byte_ptr(x86::r11, x86::rdx));
            a.jmp(readDone);

            a.bind(entrySize2);
            a.movzx(x86::eax, x86::word_ptr(x86::r11, x86::rdx));
            a.jmp(readDone);

            a.bind(entrySize3);
            a.movzx(x86::eax, x86::byte_ptr(x86::r11, x86::rdx));
            a.mov(x86::r10d, x86::eax);
            a.movzx(x86::eax, x86::word_ptr(x86::r11, x86::rdx, 0, 1));
            a.shl(x86::eax, 8);
            a.or_(x86::eax, x86::r10d);
            a.jmp(readDone);

            a.bind(entrySize4);
            a.mov(x86::eax, x86::dword_ptr(x86::r11, x86::rdx));
            a.jmp(readDone);

            a.bind(readDone);

            // Update PC with the looked-up address
            a.mov(x86::r15d, x86::eax);

            // Check if the new PC is within valid range
            a.cmp(x86::eax, codeSize);
            a.jae(pagefaultLabel);

            // Exit to interpreter with updated PC
            a.mov(x86::eax, 2);
            a.jmp(epilogueLabel);

            // No jump table available: update PC and fall back to dispatcher
            a.bind(dispatcherLoopNoJumpTable2);
            a.mov(x86::r15d, x86::eax);  // Update PC with target address
            a.jmp(dispatcherLoop);
            goto next_instruction;
        }

        // === Ecalli (Host Call) ===
        // Ecalli format: [opcode][immed_X]
        if (opcode == 10) {  // Ecalli opcode is 10
            if (instrSize < 1 || pc + instrSize > codeSize) {
                fprintf(stderr, "[JIT] Ecalli instruction truncated at PC %u (size=%u, codeSize=%zu)\n", pc, instrSize, codeSize);
                return 3;
            }

            const uint32_t l_X = clamp_imm_len(instrSize - 1);
            const uint32_t call_index = static_cast<uint32_t>(
                decode_immediate_signed(codeBuffer, pc + 1, l_X, static_cast<uint32_t>(codeSize))
            );

            // Emit host call using the pvm_host_call_trampoline
            // x86_64 calling convention:
            // rdi = context (rbp), rsi = func_idx, rdx = registers (rbx),
            // rcx = memory (r12), r8d = memory_size (r13d), r9 = gas (r14)
            a.mov(x86::rdi, x86::rbp);      // context pointer
            a.mov(x86::rsi, call_index);    // function index
            a.mov(x86::rdx, x86::rbx);      // registers pointer
            a.mov(x86::rcx, x86::r12);      // memory pointer
            a.mov(x86::r8d, x86::r13d);     // memory size
            a.mov(x86::r9, x86::r14);       // gas pointer

            // Call the trampoline
            a.mov(x86::rax, reinterpret_cast<uint64_t>(&pvm_host_call_trampoline));
            a.call(x86::rax);

            // Check result (eax contains error code or return value)
            // Any value >= 0xFFFF_FFFA is an error (hostRequestedHalt, pageFault, gasExhausted,
            // internalError, hostFunctionNotFound, hostFunctionThrewError, etc.)
            a.cmp(x86::eax, 0xFFFFFFFA);
            // Jump directly to epilogue to preserve error code in eax
            // This allows specific error codes to propagate:
            // - 0xFFFFFFFA (4294967290) = hostRequestedHalt
            // - 0xFFFFFFFB (4294967291) = pageFault
            // - 0xFFFFFFFC (4294967292) = gasExhausted
            // - 0xFFFFFFFD (4294967293) = internalError
            // - 0xFFFFFFFE (4294967294) = hostFunctionNotFound
            // - 0xFFFFFFFF (4294967295) = hostFunctionThrewError
            a.jae(epilogueLabel);  // Jump if above or equal (error range) - preserves eax

            // Store result in R0
            a.mov(x86::qword_ptr(x86::rbx, 0), x86::rax);

            goto next_instruction;
        }

        if (opcode_is(opcode, Opcode::Trap)) {
            // Set return value to -1 (trap) in eax, then jump to epilogue
            a.mov(x86::eax, -1);
            a.jmp(epilogueLabel);
            goto next_instruction;
        }

        // Opcode 1 (Halt/Fallthrough) is handled as normal instruction - just continues execution
        // Per spec, executing past program end will hit implicit Trap instructions

        // === StoreImmInd Instructions ===
        // Format: [opcode][packed_rA_lX][immed_X][immed_Y]
        if (opcode_is(opcode, Opcode::StoreImmIndU8) ||
            opcode_is(opcode, Opcode::StoreImmIndU16) ||
            opcode_is(opcode, Opcode::StoreImmIndU32) ||
            opcode_is(opcode, Opcode::StoreImmIndU64)) {
            if (instrSize < 2 || pc + instrSize > codeSize) {
                return 3;
            }

            const uint8_t packed = codeBuffer[pc + 1];
            const uint8_t base_reg = packed & 0x0F;
            const uint32_t l_X = clamp_imm_len((packed >> 4) & 0x07);
            if (instrSize < (2 + l_X)) {
                return 3;
            }
            const uint32_t l_Y = second_immediate_len(instrSize, 2, l_X);

            const int64_t offset = decode_immediate_signed(codeBuffer, pc + 2, l_X, static_cast<uint32_t>(codeSize));
            const uint64_t value = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2 + l_X, l_Y, static_cast<uint32_t>(codeSize))
            );

            // Load base address from register
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, base_reg * 8));

            // Add offset (truncate to 32-bit for parity with Swift interpreter)
            a.mov(x86::ecx, static_cast<uint32_t>(offset));  // Zero-extend to 64-bit in rcx
            a.add(x86::rax, x86::rcx);

            // Per PVM spec pvm.tex lines 137-147: first 64KB (0x10000) must cause panic
            a.cmp(x86::rax, 0x10000);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d); // memory_size
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Bitmap check: verify page is writable
            // Load writeMap from JITHostFunctionTable (RBP)
            a.mov(x86::r10, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, writeMap)));

            // If writeMap is null, skip bitmap check (allow all writes)
            a.test(x86::r10, x86::r10);
            Label skip_bitmap_check = a.new_label();
            a.jz(skip_bitmap_check);

            // Calculate page index: address / 4096 = address >> 12
            a.mov(x86::r11, x86::rax);
            a.shr(x86::r11, 12);

            // Calculate byte index: page / 8 = page >> 3
            a.mov(x86::rcx, x86::r11);
            a.shr(x86::rcx, 3);

            // Calculate bit index: page % 8 = page & 0x7
            a.and_(x86::r11d, 0x7);

            // Load bitmap byte
            a.movzx(x86::r10d, x86::byte_ptr(x86::r10, x86::rcx));

            // Test the bit using BT instruction
            a.bt(x86::r10d, x86::r11);

            // If bit is set, page is writable -> continue
            a.jc(skip_bitmap_check);

            // Page not writable -> page fault
            a.mov(x86::qword_ptr(x86::rbx, 0), x86::rax);  // Save address to VM R0
            a.jmp(pagefaultLabel);

            a.bind(skip_bitmap_check);

            // Store value
            x86::Mem mem(x86::r12, x86::rax, 0, 0);

            if (opcode_is(opcode, Opcode::StoreImmIndU8)) {
                a.mov(x86::dl, static_cast<uint8_t>(value));
                a.mov(mem, x86::dl);
            } else if (opcode_is(opcode, Opcode::StoreImmIndU16)) {
                a.mov(x86::dx, static_cast<uint16_t>(value));
                a.mov(mem, x86::dx);
            } else if (opcode_is(opcode, Opcode::StoreImmIndU32)) {
                a.mov(x86::edx, static_cast<uint32_t>(value));
                a.mov(mem, x86::edx);
            } else if (opcode_is(opcode, Opcode::StoreImmIndU64)) {
                a.mov(x86::rdx, value);
                a.mov(mem, x86::rdx);
            }

            goto next_instruction;
        }

        // === StoreInd Instructions ===
        // Format: [opcode][src_reg][base_reg][offset_32bit]
        if (opcode_is(opcode, Opcode::StoreIndU8) ||
            opcode_is(opcode, Opcode::StoreIndU16) ||
            opcode_is(opcode, Opcode::StoreIndU32) ||
            opcode_is(opcode, Opcode::StoreIndU64)) {
            
            uint8_t src_reg = codeBuffer[pc + 1];
            uint8_t base_reg = codeBuffer[pc + 2];
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 3], 4);

            // Load base address from register
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, base_reg * 8));

            // Add offset (zero-extend to 64-bit by loading into 32-bit register first)
            a.mov(x86::r8d, offset);  // Use r8d (32-bit) to ensure zero-extension
            a.add(x86::rax, x86::r8);

            // Per PVM spec pvm.tex lines 137-147: first 64KB (0x10000) must cause panic
            a.cmp(x86::rax, 0x10000);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d); // memory_size
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Bitmap check: verify page is writable
            a.mov(x86::r10, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, writeMap)));
            a.test(x86::r10, x86::r10);
            Label skip_bitmap_check2 = a.new_label();
            a.jz(skip_bitmap_check2);

            a.mov(x86::r11, x86::rax);
            a.shr(x86::r11, 12);
            a.mov(x86::rcx, x86::r11);
            a.shr(x86::rcx, 3);
            a.and_(x86::r11d, 0x7);
            a.movzx(x86::r10d, x86::byte_ptr(x86::r10, x86::rcx));
            a.bt(x86::r10d, x86::r11);
            a.jc(skip_bitmap_check2);

            a.mov(x86::qword_ptr(x86::rbx, 0), x86::rax);
            a.jmp(pagefaultLabel);

            a.bind(skip_bitmap_check2);

            // Load source value
            a.mov(x86::rdx, x86::qword_ptr(x86::rbx, src_reg * 8));
            
            // Store value
            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            
            if (opcode_is(opcode, Opcode::StoreIndU8)) {
                a.mov(mem, x86::dl);
            } else if (opcode_is(opcode, Opcode::StoreIndU16)) {
                a.mov(mem, x86::dx);
            } else if (opcode_is(opcode, Opcode::StoreIndU32)) {
                a.mov(mem, x86::edx);
            } else if (opcode_is(opcode, Opcode::StoreIndU64)) {
                a.mov(mem, x86::rdx);
            }
            
            goto next_instruction;
        }

        // === LoadInd Instructions ===
        // Format: [opcode][dest_reg][base_reg][offset_32bit]
        if (opcode_is(opcode, Opcode::LoadIndU8) ||
            opcode_is(opcode, Opcode::LoadIndI8) ||
            opcode_is(opcode, Opcode::LoadIndU16) ||
            opcode_is(opcode, Opcode::LoadIndI16) ||
            opcode_is(opcode, Opcode::LoadIndU32) ||
            opcode_is(opcode, Opcode::LoadIndI32) ||
            opcode_is(opcode, Opcode::LoadIndU64)) {

            uint8_t dest_reg = codeBuffer[pc + 1];
            uint8_t base_reg = codeBuffer[pc + 2];
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 3], 4);

            // Load base address from register
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, base_reg * 8));

            // Add offset (zero-extend to 64-bit by loading into 32-bit register first)
            a.mov(x86::r8d, offset);  // Use r8d (32-bit) to ensure zero-extension
            a.add(x86::rax, x86::r8);

            // Bounds check: address < 65536 → panic
            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d); // memory_size
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);
            
            // Load value
            if (opcode_is(opcode, Opcode::LoadIndU8)) {
                a.movzx(x86::ecx, x86::byte_ptr(x86::r12, x86::rax));
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rcx);
            } else if (opcode_is(opcode, Opcode::LoadIndI8)) {
                a.movsx(x86::rcx, x86::byte_ptr(x86::r12, x86::rax));
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rcx);
            } else if (opcode_is(opcode, Opcode::LoadIndU16)) {
                a.movzx(x86::ecx, x86::word_ptr(x86::r12, x86::rax));
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rcx);
            } else if (opcode_is(opcode, Opcode::LoadIndI16)) {
                a.movsx(x86::rcx, x86::word_ptr(x86::r12, x86::rax));
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rcx);
            } else if (opcode_is(opcode, Opcode::LoadIndU32)) {
                a.mov(x86::ecx, x86::dword_ptr(x86::r12, x86::rax, 0, 0)); // 32-bit mov zero-extends
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rcx);
            } else if (opcode_is(opcode, Opcode::LoadIndI32)) {
                a.movsxd(x86::rcx, x86::dword_ptr(x86::r12, x86::rax, 0, 0)); // Sign-extend 32 to 64
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rcx);
            } else if (opcode_is(opcode, Opcode::LoadIndU64)) {
                a.mov(x86::rcx, x86::qword_ptr(x86::r12, x86::rax, 0, 0));
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rcx);
            }
            
            goto next_instruction;
        }

        // === Division Instructions with Zero-Check and Overflow-Check ===
        // Division by zero causes hardware exception - must check explicitly
        // Signed division overflow (INT_MIN / -1) causes hardware exception - must check explicitly
        // Format: [opcode][dest_reg][src_reg] = 3 bytes
        if (opcode_is(opcode, Opcode::DivU32) ||
            opcode_is(opcode, Opcode::DivS32) ||
            opcode_is(opcode, Opcode::RemU32) ||
            opcode_is(opcode, Opcode::RemS32) ||
            opcode_is(opcode, Opcode::DivU64) ||
            opcode_is(opcode, Opcode::DivS64) ||
            opcode_is(opcode, Opcode::RemU64) ||
            opcode_is(opcode, Opcode::RemS64)) {
            // Decode instruction: [opcode][ra | (rb << 4)][rd]
            uint8_t ra = codeBuffer[pc + 1] & 0x0F;           // ra (lower 4 bits)
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;    // rb (upper 4 bits)
            uint8_t rd = codeBuffer[pc + 2];                  // rd

            // Label to skip all division handling and continue
            Label divisionDone = a.new_label();

            // Load divisor (rb) into rcx
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // For remainder instructions, check if divisor is zero and return dividend
            if (opcode_is(opcode, Opcode::RemU32) || opcode_is(opcode, Opcode::RemU64) ||
                opcode_is(opcode, Opcode::RemS32) || opcode_is(opcode, Opcode::RemS64)) {
                Label divisorNotZero = a.new_label();

                a.test(x86::rcx, x86::rcx);
                a.jnz(divisorNotZero);  // If divisor != 0, continue with division

                // Division by zero case for remainder: return dividend (ra)
                if (opcode_is(opcode, Opcode::RemU32) || opcode_is(opcode, Opcode::RemS32)) {
                    a.mov(x86::eax, x86::dword_ptr(x86::rbx, ra * 8));
                    a.movsxd(x86::rax, x86::eax);
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                } else {
                    // RemU64/RemS64
                    a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                }
                a.jmp(divisionDone);  // Skip all division handling and continue

                a.bind(divisorNotZero);
                // Continue with normal division below
            }

            // For signed division, check if divisor is zero and return UInt64.max
            if (opcode_is(opcode, Opcode::DivS32) || opcode_is(opcode, Opcode::DivS64)) {
                Label divisorNotZero = a.new_label();

                a.test(x86::rcx, x86::rcx);
                a.jnz(divisorNotZero);  // If divisor != 0, continue with division

                // Division by zero case: return UInt64.max
                a.mov(x86::rax, 0xFFFFFFFFFFFFFFFFULL);
                a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                a.jmp(divisionDone);  // Skip all division handling and continue

                a.bind(divisorNotZero);
                // Continue with normal division below
            }

            // For unsigned division (DivU32/DivU64), check if divisor is zero and return UInt64.max
            if (opcode_is(opcode, Opcode::DivU32) || opcode_is(opcode, Opcode::DivU64)) {
                Label divisorNotZero = a.new_label();

                a.test(x86::rcx, x86::rcx);
                a.jnz(divisorNotZero);  // If divisor != 0, continue with division

                // Division by zero case: return UInt64.max
                a.mov(x86::rax, 0xFFFFFFFFFFFFFFFFULL);
                a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                a.jmp(divisionDone);  // Skip all division handling and continue

                a.bind(divisorNotZero);
                // Continue with normal division below
            }

            // Handle Signed Division Overflow
            if (opcode_is(opcode, Opcode::DivS32) || opcode_is(opcode, Opcode::RemS32)) {
                Label noOverflow = a.new_label();
                Label divDone = a.new_label();
                
                // Check divisor == -1 (in ecx)
                a.cmp(x86::ecx, -1);
                a.jne(noOverflow);
                
                // Load dividend (ra) into eax
                a.mov(x86::eax, x86::dword_ptr(x86::rbx, ra * 8));

                // Check dividend == INT32_MIN (0x80000000)
                a.cmp(x86::eax, 0x80000000);
                a.jne(noOverflow);

                // Overflow case detected
                if (opcode_is(opcode, Opcode::DivS32)) {
                    // Result is INT32_MIN (already in eax). Sign-extend to 64-bit and store.
                    a.movsxd(x86::rax, x86::eax);
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                } else {
                    // RemS32: Result is 0
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), 0);
                }
                a.jmp(divDone);

                a.bind(noOverflow);
                // Normal division
                // Load dividend (ra) into eax again (in case we skipped the load above)
                a.mov(x86::eax, x86::dword_ptr(x86::rbx, ra * 8));
                a.cdq(); // Sign extend eax -> edx:eax
                a.idiv(x86::ecx);

                // Store result
                if (opcode_is(opcode, Opcode::DivS32)) {
                    a.movsxd(x86::rax, x86::eax); // Sign extend result
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                } else {
                    a.movsxd(x86::rax, x86::edx); // Sign extend remainder
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                }

                a.bind(divDone);
            } else if (opcode_is(opcode, Opcode::DivS64) || opcode_is(opcode, Opcode::RemS64)) {
                Label noOverflow = a.new_label();
                Label divDone = a.new_label();

                // Check divisor == -1 (in rcx)
                a.cmp(x86::rcx, -1);
                a.jne(noOverflow);

                // Load dividend (ra) into rax
                a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));

                // Check dividend == INT64_MIN (0x8000000000000000)
                // We can't use 64-bit immediate with cmp directly if it doesn't fit in 32-bit signed
                a.mov(x86::rdx, 0x8000000000000000);
                a.cmp(x86::rax, x86::rdx);
                a.jne(noOverflow);

                // Overflow case detected
                if (opcode_is(opcode, Opcode::DivS64)) {
                    // Result is INT64_MIN (already in rax). Store it.
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                } else {
                    // RemS64: Result is 0
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), 0);
                }
                a.jmp(divDone);

                a.bind(noOverflow);
                // Normal division
                a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
                a.cqo(); // Sign extend rax -> rdx:rax
                a.idiv(x86::rcx);

                // Store result
                if (opcode_is(opcode, Opcode::DivS64)) {
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                } else {
                    a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rdx);
                }

                a.bind(divDone);
            } else {
                // Unsigned Division (DivU32, RemU32, DivU64, RemU64)
                // No overflow check needed for unsigned
                
                if (opcode_is(opcode, Opcode::DivU32) || opcode_is(opcode, Opcode::RemU32)) {
                    a.mov(x86::eax, x86::dword_ptr(x86::rbx, ra * 8));  // Load dividend (ra)
                    a.xor_(x86::edx, x86::edx); // Zero extend
                    a.div(x86::ecx);

                    if (opcode_is(opcode, Opcode::DivU32)) {
                        a.mov(x86::ecx, x86::eax); // Zero extend result
                        a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rcx);
                    } else {
                        a.mov(x86::ecx, x86::edx); // Zero extend remainder
                        a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rcx);
                    }
                } else {
                    // 64-bit unsigned
                    a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));  // Load dividend (ra)
                    a.xor_(x86::edx, x86::edx); // Zero extend
                    a.div(x86::rcx);

                    if (opcode_is(opcode, Opcode::DivU64)) {
                        a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);
                    } else {
                        a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rdx);
                    }
                }
            }

            a.bind(divisionDone);

            goto next_instruction;
        }

        // === Load Instructions with Bounds Checking ===
        // PVM Spec: addresses < 0x10000 (65536) → panic, addresses >= memory_size → page fault
        // Format: [opcode][reg_index][address_32bit] (6 bytes)
        if (opcode_is(opcode, Opcode::LoadU8) ||
            opcode_is(opcode, Opcode::LoadI8) ||
            opcode_is(opcode, Opcode::LoadU16) ||
            opcode_is(opcode, Opcode::LoadI16) ||
            opcode_is(opcode, Opcode::LoadU32) ||
            opcode_is(opcode, Opcode::LoadI32) ||
            opcode_is(opcode, Opcode::LoadU64)) {
            // Decode instruction: [opcode][dest_reg][address_32bit]
            uint8_t dest_reg = codeBuffer[pc + 1];
            uint32_t address;
            memcpy(&address, &codeBuffer[pc + 2], 4);

            // Load address into rax (zero-extended from 32-bit)
            // mov to 64-bit register zero-extends automatically
            a.mov(x86::rax, uint64_t(address));

            // Bounds check: address < 0x10000 → panic (reserved region)
            a.cmp(x86::rax, 0x10000);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.cmp(x86::rax, x86::r13);  // Compare with full 64-bit memory_size
            a.jae(pagefaultLabel);

            // Bitmap check: verify page is readable
            a.mov(x86::r10, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, readMap)));
            a.test(x86::r10, x86::r10);
            Label skip_load_bitmap = a.new_label();
            a.jz(skip_load_bitmap);

            a.mov(x86::r11, x86::rax);
            a.shr(x86::r11, 12);
            a.mov(x86::rcx, x86::r11);
            a.shr(x86::rcx, 3);
            a.and_(x86::r11d, 0x7);
            a.movzx(x86::r10d, x86::byte_ptr(x86::r10, x86::rcx));
            a.bt(x86::r10d, x86::r11);
            a.jc(skip_load_bitmap);

            a.mov(x86::qword_ptr(x86::rbx, 0), x86::rax);
            a.jmp(pagefaultLabel);

            a.bind(skip_load_bitmap);

            // If we get here, address is valid - inline the load instruction
            // Load from memory into register, then store to VM register
            x86::Mem mem(x86::r12, x86::rax, 0, 0);

            if (opcode_is(opcode, Opcode::LoadU8)) {
                a.movzx(x86::eax, x86::byte_ptr(x86::r12, x86::rax));  // Zero-extend byte to 32-bit
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rax);
            } else if (opcode_is(opcode, Opcode::LoadI8)) {
                a.movsx(x86::rax, x86::byte_ptr(x86::r12, x86::rax));  // Sign-extend byte to 64-bit
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rax);
            } else if (opcode_is(opcode, Opcode::LoadU16)) {
                a.movzx(x86::eax, x86::word_ptr(x86::r12, x86::rax));  // Zero-extend word to 32-bit
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rax);
            } else if (opcode_is(opcode, Opcode::LoadI16)) {
                a.movsx(x86::rax, x86::word_ptr(x86::r12, x86::rax));  // Sign-extend word to 64-bit
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rax);
            } else if (opcode_is(opcode, Opcode::LoadU32)) {
                a.mov(x86::eax, x86::dword_ptr(x86::r12, x86::rax, 0, 0));  // 32-bit mov zero-extends automatically
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rax);
            } else if (opcode_is(opcode, Opcode::LoadI32)) {
                a.movsxd(x86::rax, x86::dword_ptr(x86::r12, x86::rax, 0, 0));  // Sign-extend
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rax);
            } else if (opcode_is(opcode, Opcode::LoadU64)) {
                a.mov(x86::rax, x86::qword_ptr(x86::r12, x86::rax, 0, 0));
                a.mov(x86::qword_ptr(x86::rbx, dest_reg * 8), x86::rax);
            }

            goto next_instruction;
        }

        // === Store Instructions with Bounds Checking ===
        // NOTE: StoreU8/16/32 use direct addressing: [opcode][reg][address_32bit]
        // StoreU64 is handled separately below with correct direct addressing format
        if (opcode_is(opcode, Opcode::StoreU8) ||
            opcode_is(opcode, Opcode::StoreU16) ||
            opcode_is(opcode, Opcode::StoreU32)) {
            // Decode instruction: [opcode][reg_index][address_32bit]
            uint8_t srcReg = codeBuffer[pc + 1];
            uint32_t address;
            memcpy(&address, &codeBuffer[pc + 2], 4);

            // Load address into rax (zero-extended from 32-bit)
            // mov to 64-bit register zero-extends automatically
            a.mov(x86::rax, uint64_t(address));

            // Per PVM spec pvm.tex lines 137-147: first 64KB (0x10000) must cause panic
            a.cmp(x86::rax, 0x10000);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.cmp(x86::rax, x86::r13);  // Compare with full 64-bit memory_size
            a.jae(pagefaultLabel);

            // Bitmap check: verify page is writable
            a.mov(x86::r10, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, writeMap)));
            a.test(x86::r10, x86::r10);
            Label skip_store_bitmap = a.new_label();
            a.jz(skip_store_bitmap);

            a.mov(x86::r11, x86::rax);
            a.shr(x86::r11, 12);
            a.mov(x86::rcx, x86::r11);
            a.shr(x86::rcx, 3);
            a.and_(x86::r11d, 0x7);
            a.movzx(x86::r10d, x86::byte_ptr(x86::r10, x86::rcx));
            a.bt(x86::r10d, x86::r11);
            a.jc(skip_store_bitmap);

            a.mov(x86::qword_ptr(x86::rbx, 0), x86::rax);
            a.jmp(pagefaultLabel);

            a.bind(skip_store_bitmap);

            // Load source value from VM register
            a.mov(x86::rdx, x86::qword_ptr(x86::rbx, srcReg * 8));

            // Store to memory [VM_MEMORY_PTR + address]
            x86::Mem mem(x86::r12, x86::rax, 0, 0);

            if (opcode_is(opcode, Opcode::StoreU8)) {
                a.mov(mem, x86::dl);
            } else if (opcode_is(opcode, Opcode::StoreU16)) {
                a.mov(mem, x86::dx);
            } else if (opcode_is(opcode, Opcode::StoreU32)) {
                a.mov(mem, x86::edx);
            }

            goto next_instruction;
        }

        // === StoreImm Instructions with Bounds Checking ===
        // PVM spec format: [opcode][len][immed_X][immed_Y]
        if (opcode_is(opcode, Opcode::StoreImmU8)) {
            if (instrSize < 2 || pc + instrSize > codeSize) {
                return 3;
            }
            uint8_t len_byte = codeBuffer[pc + 1];
            uint32_t l_X = clamp_imm_len(len_byte & 0x07);
            if (instrSize < (2 + l_X)) {
                return 3;
            }
            uint32_t l_Y = second_immediate_len(instrSize, 2, l_X);
            uint64_t address = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2, l_X, static_cast<uint32_t>(codeSize))
            );
            uint64_t value = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2 + l_X, l_Y, static_cast<uint32_t>(codeSize))
            );

            // Load address into rax (zero-extended from 32-bit)
            // mov to 64-bit register zero-extends automatically
            a.mov(x86::rax, address);

            // Bounds check: address < 65536 → panic
            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d);  // Load memory_size into ecx
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Store value to memory
            a.mov(x86::cl, static_cast<uint8_t>(value));
            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            a.mov(mem, x86::cl);

            goto next_instruction;
        }

        if (opcode_is(opcode, Opcode::StoreImmU16)) {
            if (instrSize < 2 || pc + instrSize > codeSize) {
                return 3;
            }
            uint8_t len_byte = codeBuffer[pc + 1];
            uint32_t l_X = clamp_imm_len(len_byte & 0x07);
            if (instrSize < (2 + l_X)) {
                return 3;
            }
            uint32_t l_Y = second_immediate_len(instrSize, 2, l_X);
            uint64_t address = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2, l_X, static_cast<uint32_t>(codeSize))
            );
            uint64_t value = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2 + l_X, l_Y, static_cast<uint32_t>(codeSize))
            );

            // Load address into rax (zero-extended from 32-bit)
            // mov to 64-bit register zero-extends automatically
            a.mov(x86::rax, address);

            // Bounds check: address < 65536 → panic
            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d);  // Load memory_size into ecx
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Store value to memory
            a.mov(x86::cx, static_cast<uint16_t>(value));
            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            a.mov(mem, x86::cx);

            goto next_instruction;
        }

        if (opcode_is(opcode, Opcode::StoreImmU32)) {
            if (instrSize < 2 || pc + instrSize > codeSize) {
                return 3;
            }
            uint8_t len_byte = codeBuffer[pc + 1];
            uint32_t l_X = clamp_imm_len(len_byte & 0x07);
            if (instrSize < (2 + l_X)) {
                return 3;
            }
            uint32_t l_Y = second_immediate_len(instrSize, 2, l_X);
            uint64_t address = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2, l_X, static_cast<uint32_t>(codeSize))
            );
            uint64_t value = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2 + l_X, l_Y, static_cast<uint32_t>(codeSize))
            );

            // Load address into rax (zero-extended from 32-bit)
            // mov to 64-bit register zero-extends automatically
            a.mov(x86::rax, address);

            // Bounds check: address < 65536 → panic
            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d);  // Load memory_size into ecx
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Store value to memory
            a.mov(x86::ecx, static_cast<uint32_t>(value));
            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            a.mov(mem, x86::ecx);

            goto next_instruction;
        }

        if (opcode_is(opcode, Opcode::StoreImmU64)) {
            if (instrSize < 2 || pc + instrSize > codeSize) {
                return 3;
            }
            uint8_t len_byte = codeBuffer[pc + 1];
            uint32_t l_X = clamp_imm_len(len_byte & 0x07);
            if (instrSize < (2 + l_X)) {
                return 3;
            }
            uint32_t l_Y = second_immediate_len(instrSize, 2, l_X);
            uint64_t address = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2, l_X, static_cast<uint32_t>(codeSize))
            );
            uint64_t value = static_cast<uint64_t>(
                decode_immediate_signed(codeBuffer, pc + 2 + l_X, l_Y, static_cast<uint32_t>(codeSize))
            );

            // Load address into rax (zero-extended from 32-bit)
            // mov to 64-bit register zero-extends automatically
            a.mov(x86::rax, address);

            // Bounds check: address < 65536 → panic
            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d);  // Load memory_size into ecx
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Store value to memory
            a.mov(x86::rcx, value);
            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            a.mov(mem, x86::rcx);

            goto next_instruction;
        }

        // === LoadIndU32: Load 32-bit unsigned value from indirect address ===
        // Format: [opcode][ra_rb][offset_32bit] where ra_rb = (ra | rb << 4)
        if (opcode_is(opcode, Opcode::LoadIndU32)) {
            uint8_t ra_rb = codeBuffer[pc + 1];
            uint8_t ra = (ra_rb >> 0) & 0x0F;
            uint8_t rb = (ra_rb >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            // DEBUG: Log instruction details

            // Load base address from rb register into rax
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));

            // DEBUG: Log loaded base address (need to save rax first)
            a.mov(x86::rdi, x86::rax);  // Save rax to rdi
            // We can't easily call fprintf here, so we'll skip runtime logging

            // Add offset to get final address
            a.add(x86::rax, offset);

            // Bounds check: address < 65536 → panic
            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Load 32-bit value from memory into eax (zero-extended to rax)
            a.mov(x86::eax, x86::dword_ptr(x86::r12, x86::rax));
            a.movsxd(x86::rax, x86::eax);

            // Store to destination register ra
            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // === LoadIndI32: Load 32-bit signed value from indirect address ===
        if (opcode_is(opcode, Opcode::LoadIndI32)) {
            uint8_t ra_rb = codeBuffer[pc + 1];
            uint8_t ra = (ra_rb >> 0) & 0x0F;
            uint8_t rb = (ra_rb >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Load 32-bit signed value (sign-extends to 64-bit)
            a.movsxd(x86::rax, x86::dword_ptr(x86::r12, x86::rax));

            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // === LoadIndU16: Load 16-bit unsigned value from indirect address ===
        if (opcode_is(opcode, Opcode::LoadIndU16)) {
            uint8_t ra_rb = codeBuffer[pc + 1];
            uint8_t ra = (ra_rb >> 0) & 0x0F;
            uint8_t rb = (ra_rb >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Load 16-bit unsigned value (zero-extended to 64-bit)
            a.movzx(x86::eax, x86::word_ptr(x86::r12, x86::rax));
            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // === LoadIndI16: Load 16-bit signed value from indirect address ===
        if (opcode_is(opcode, Opcode::LoadIndI16)) {
            uint8_t ra_rb = codeBuffer[pc + 1];
            uint8_t ra = (ra_rb >> 0) & 0x0F;
            uint8_t rb = (ra_rb >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Load 16-bit signed value (sign-extends to 64-bit)
            a.movsx(x86::rax, x86::word_ptr(x86::r12, x86::rax));

            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // === LoadIndU8: Load 8-bit unsigned value from indirect address ===
        if (opcode_is(opcode, Opcode::LoadIndU8)) {
            uint8_t ra_rb = codeBuffer[pc + 1];
            uint8_t ra = (ra_rb >> 0) & 0x0F;
            uint8_t rb = (ra_rb >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Load 8-bit unsigned value (zero-extended to 64-bit)
            a.movzx(x86::eax, x86::byte_ptr(x86::r12, x86::rax));
            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // === LoadIndI8: Load 8-bit signed value from indirect address ===
        if (opcode_is(opcode, Opcode::LoadIndI8)) {
            uint8_t ra_rb = codeBuffer[pc + 1];
            uint8_t ra = (ra_rb >> 0) & 0x0F;
            uint8_t rb = (ra_rb >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Load 8-bit signed value (sign-extends to 64-bit)
            a.movsx(x86::rax, x86::byte_ptr(x86::r12, x86::rax));

            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // === LoadIndU64: Load 64-bit unsigned value from indirect address ===
        if (opcode_is(opcode, Opcode::LoadIndU64)) {
            uint8_t ra_rb = codeBuffer[pc + 1];
            uint8_t ra = (ra_rb >> 0) & 0x0F;
            uint8_t rb = (ra_rb >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Load 64-bit value
            a.mov(x86::rax, x86::qword_ptr(x86::r12, x86::rax));

            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // === StoreIndU8: Store 8-bit value to indirect address ===
        // Format: [opcode][src_dest][offset_32bit] where src_dest = (src | dest << 4)
        if (opcode_is(opcode, Opcode::StoreIndU8)) {
            uint8_t src_dest = codeBuffer[pc + 1];
            uint8_t src = (src_dest >> 0) & 0x0F;
            uint8_t dest = (src_dest >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            // Load base address from dest register into rax
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, dest * 8));

            // Add offset
            a.add(x86::rax, offset);

            // Bounds check: address < 65536 → panic
            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            // Load value from src register into cl (lower 8 bits)
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, src * 8));

            // Store 8-bit value to memory
            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            a.mov(mem, x86::cl);

            goto next_instruction;
        }

        // === StoreIndU16: Store 16-bit value to indirect address ===
        if (opcode_is(opcode, Opcode::StoreIndU16)) {
            uint8_t src_dest = codeBuffer[pc + 1];
            uint8_t src = (src_dest >> 0) & 0x0F;
            uint8_t dest = (src_dest >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, dest * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, src * 8));

            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            a.mov(mem, x86::cx);

            goto next_instruction;
        }

        // === StoreIndU32: Store 32-bit value to indirect address ===
        if (opcode_is(opcode, Opcode::StoreIndU32)) {
            uint8_t src_dest = codeBuffer[pc + 1];
            uint8_t src = (src_dest >> 0) & 0x0F;
            uint8_t dest = (src_dest >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, dest * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            a.mov(x86::ecx, x86::qword_ptr(x86::rbx, src * 8));

            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            a.mov(mem, x86::ecx);

            goto next_instruction;
        }

        // === StoreIndU64: Store 64-bit value to indirect address ===
        if (opcode_is(opcode, Opcode::StoreIndU64)) {
            uint8_t src_dest = codeBuffer[pc + 1];
            uint8_t src = (src_dest >> 0) & 0x0F;
            uint8_t dest = (src_dest >> 4) & 0x0F;
            uint32_t offset;
            memcpy(&offset, &codeBuffer[pc + 2], 4);

            a.mov(x86::rax, x86::qword_ptr(x86::rbx, dest * 8));
            a.add(x86::rax, offset);

            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            a.mov(x86::ecx, x86::r13d);
            a.cmp(x86::rax, x86::rcx);
            a.jae(pagefaultLabel);

            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, src * 8));

            x86::Mem mem(x86::r12, x86::rax, 0, 0);
            a.mov(mem, x86::rcx);

            goto next_instruction;
        }

        // LoadImm: Load 32-bit immediate into register
        if (opcode_is(opcode, Opcode::LoadImm)) {
            // Per spec pvm.tex lines 327-334:
            // Format: [opcode][reg_index][immed_X (l_X bytes)]
            // where:
            //   ℓ = instruction length in bytes (from skip table)
            //   l_X = min(4, ℓ - 1)
            //   immed_X is sign-extended from l_X bytes to 64 bits
            //
            // Examples:
            //   ℓ=3 → l_X=2 → [opcode][reg][imm16] (3 bytes total)
            //   ℓ=4 → l_X=3 → [opcode][reg][imm24] (4 bytes total)
            //   ℓ=5+ → l_X=4 → [opcode][reg][imm32] (5+ bytes total)
            //
            // CRITICAL: ℓ comes from skip table, NOT from remaining bytecode length!

            // Get instruction length from skip table
            uint32_t instrLen = getInstrSize(pc);

            // Validate minimum length (opcode + register byte)
            if (instrLen < 2) {
                fprintf(stderr, "[JIT] LoadImm instruction too short at PC %u (length=%u)\n", pc, instrLen);
                return 3; // Compilation error
            }

            // Validate we have the full instruction in bytecode
            if (pc + instrLen > codeSize) {
                fprintf(stderr, "[JIT] LoadImm instruction truncated at PC %u (need %u bytes, have %zu)\n",
                        pc, instrLen, codeSize);
                return 3; // Compilation error
            }

            uint8_t destReg = codeBuffer[pc + 1];

            // Calculate l_X from instruction length per spec
            uint32_t l_X = (instrLen > 2) ? clamp_imm_len(instrLen - 2) : 0;

            // Validate we have enough bytes for the immediate
            if (pc + 2 + l_X > codeSize) {
                fprintf(stderr, "[JIT] LoadImm immediate truncated at PC %u (l_X=%u)\n", pc, l_X);
                return 3; // Compilation error
            }

            // Load immediate value with l_X bytes
            uint32_t immediate = 0;
            memcpy(&immediate, &codeBuffer[pc + 2], l_X);

            // Sign-extend based on l_X
            if (l_X == 1) {
                immediate = int32_t(int8_t(immediate));
            } else if (l_X == 2) {
                immediate = int32_t(int16_t(immediate));
            } else if (l_X == 3) {
                // Sign-extend 24-bit to 32-bit
                if (immediate & 0x800000) {
                    immediate |= 0xFF000000;
                }
            }
            // l_X == 4: no sign-extension needed (already 32-bit)

            // LoadImm loads signed values and sign-extends to 64-bit
            a.mov(x86::eax, immediate);
            a.movsxd(x86::rax, x86::eax);  // Sign-extend eax to rax
            a.mov(x86::qword_ptr(x86::rbx, destReg * 8), x86::rax);

            pc += instrLen;
            instructionHandled = true;
            goto next_instruction;
        }

        // LoadImmU64: Load 64-bit immediate into register
        if (opcode_is(opcode, Opcode::LoadImmU64)) {
            // LoadImmU64: [opcode][reg_index][value_64bit] = 10 bytes
            uint8_t destReg = codeBuffer[pc + 1];

            // Load 64-bit immediate value from bytecode
            uint64_t immediate;
            memcpy(&immediate, &codeBuffer[pc + 2], 8);

            // Load immediate into rax and store to VM register
            a.mov(x86::rax, immediate);
            a.mov(x86::qword_ptr(x86::rbx, destReg * 8), x86::rax);

            goto next_instruction;
        }

        // Add64: 64-bit addition
        if (opcode_is(opcode, Opcode::Add64)) {
            // Add64: [opcode][ra|rb<<4][rd] = 4 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // Add
            a.add(x86::rax, x86::rcx);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // Add32: 32-bit addition
        if (opcode_is(opcode, Opcode::Add32)) {
            // Add32: [opcode][ra|rb<<4][rd] = 4 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and rb as 32-bit values
            a.mov(x86::eax, x86::dword_ptr(x86::rbx, ra * 8));
            a.mov(x86::ecx, x86::dword_ptr(x86::rbx, rb * 8));

            // Add (32-bit with truncation on overflow)
            a.add(x86::eax, x86::ecx);

            // Sign-extend result to 64-bit
            a.movsxd(x86::rax, x86::eax);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // Sub64: 64-bit subtraction
        if (opcode_is(opcode, Opcode::Sub64)) {
            // Sub64: [opcode][ra|rb<<4][rd] = 4 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // Subtract
            a.sub(x86::rax, x86::rcx);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // AddImm64: Add 64-bit immediate to register
        if (opcode_is(opcode, Opcode::AddImm64)) {
            // AddImm64: [opcode][packed_ra_rb][value_le32] = 6 bytes
            // where packed_ra_rb = (rb << 4) | ra
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;  // dest register
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;  // source register

            // Decode 32-bit little-endian immediate and sign-extend to 64-bit
            int32_t imm32;
            memcpy(&imm32, &codeBuffer[pc + 2], 4);

            // Load rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));

            // Add sign-extended 32-bit immediate (add with sign-extension is automatic)
            a.add(x86::rax, imm32);

            // Store to ra
            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // MulImm64: Multiply 64-bit immediate with register
        if (opcode_is(opcode, Opcode::MulImm64)) {
            // MulImm64: [opcode][packed_ra_rb][value_le32] = 6 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;  // dest register
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;  // source register

            // Decode 32-bit little-endian immediate and sign-extend to 64-bit
            int32_t imm32;
            memcpy(&imm32, &codeBuffer[pc + 2], 4);

            // Load rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));

            // Multiply with sign-extended 32-bit immediate
            // imul r64, imm32 sign-extends the immediate
            a.imul(x86::rax, imm32);

            // Store to ra
            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // ShloLImm64: Shift left by immediate
        if (opcode_is(opcode, Opcode::ShloLImm64)) {
            // ShloLImm64: [opcode][packed_ra_rb][value_le32] = 6 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;  // dest register
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;  // source register

            // Decode 32-bit little-endian immediate and sign-extend to 64-bit
            int32_t imm32;
            memcpy(&imm32, &codeBuffer[pc + 2], 4);
            int64_t imm64 = imm32;  // Sign-extend

            // Load rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));

            // Shift left by immediate (modulo 64)
            a.mov(x86::rcx, imm64);
            a.and_(x86::rcx, 63);  // Mask to 0-63
            a.shl(x86::rax, x86::cl);

            // Store to ra
            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // ShloRImm64: Shift right logical by immediate
        if (opcode_is(opcode, Opcode::ShloRImm64)) {
            // ShloRImm64: [opcode][packed_ra_rb][value_le32] = 6 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;  // dest register
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;  // source register

            // Decode 32-bit little-endian immediate and sign-extend to 64-bit
            int32_t imm32;
            memcpy(&imm32, &codeBuffer[pc + 2], 4);
            int64_t imm64 = imm32;  // Sign-extend

            // Load rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));

            // Shift right logical by immediate (modulo 64)
            a.mov(x86::rcx, imm64);
            a.and_(x86::rcx, 63);  // Mask to 0-63
            a.shr(x86::rax, x86::cl);

            // Store to ra
            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // SharRImm64: Shift right arithmetic by immediate
        if (opcode_is(opcode, Opcode::SharRImm64)) {
            // SharRImm64: [opcode][packed_ra_rb][value_le32] = 6 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;  // dest register
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;  // source register

            // Decode 32-bit little-endian immediate and sign-extend to 64-bit
            int32_t imm32;
            memcpy(&imm32, &codeBuffer[pc + 2], 4);
            int64_t imm64 = imm32;  // Sign-extend

            // Load rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, rb * 8));

            // Shift right arithmetic by immediate (modulo 64)
            a.mov(x86::rcx, imm64);
            a.and_(x86::rcx, 63);  // Mask to 0-63
            a.sar(x86::rax, x86::cl);

            // Store to ra
            a.mov(x86::qword_ptr(x86::rbx, ra * 8), x86::rax);

            goto next_instruction;
        }

        // ShloL64: Shift left by register (3-register format)
        if (opcode_is(opcode, Opcode::ShloL64)) {
            // ShloL64: [opcode][ra|rb<<4][rd] = 3 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and shift amount rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // Mask shift amount to 0-63
            a.and_(x86::rcx, 63);

            // Shift left
            a.shl(x86::rax, x86::cl);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // ShloR64: Shift right logical by register
        if (opcode_is(opcode, Opcode::ShloR64)) {
            // ShloR64: [opcode][ra|rb<<4][rd] = 3 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and shift amount rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // Mask shift amount to 0-63
            a.and_(x86::rcx, 63);

            // Shift right logical
            a.shr(x86::rax, x86::cl);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // SharR64: Shift right arithmetic by register
        if (opcode_is(opcode, Opcode::SharR64)) {
            // SharR64: [opcode][ra|rb<<4][rd] = 3 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and shift amount rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // Mask shift amount to 0-63
            a.and_(x86::rcx, 63);

            // Shift right arithmetic (preserves sign)
            a.sar(x86::rax, x86::cl);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // And: Bitwise AND
        if (opcode_is(opcode, Opcode::And)) {
            // And: [opcode][ra|rb<<4][rd] = 3 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // Bitwise AND
            a.and_(x86::rax, x86::rcx);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // Or: Bitwise OR
        if (opcode_is(opcode, Opcode::Or)) {
            // Or: [opcode][ra|rb<<4][rd] = 3 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // Bitwise OR
            a.or_(x86::rax, x86::rcx);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // Xor: Bitwise XOR
        if (opcode_is(opcode, Opcode::Xor)) {
            // Xor: [opcode][ra|rb<<4][rd] = 3 bytes
            uint8_t ra = (codeBuffer[pc + 1] >> 0) & 0x0F;
            uint8_t rb = (codeBuffer[pc + 1] >> 4) & 0x0F;
            uint8_t rd = codeBuffer[pc + 2];

            // Load ra and rb
            a.mov(x86::rax, x86::qword_ptr(x86::rbx, ra * 8));
            a.mov(x86::rcx, x86::qword_ptr(x86::rbx, rb * 8));

            // Bitwise XOR
            a.xor_(x86::rax, x86::rcx);

            // Store to rd
            a.mov(x86::qword_ptr(x86::rbx, rd * 8), x86::rax);

            goto next_instruction;
        }

        // StoreU64: Store 64-bit value from register to memory
        if (opcode_is(opcode, Opcode::StoreU64)) {
            // StoreU64: [opcode][reg_index][address_32bit] = 6 bytes
            uint8_t srcReg = codeBuffer[pc + 1];
            uint32_t address;
            memcpy(&address, &codeBuffer[pc + 2], 4);

            // Load address into eax (zero-extends to rax automatically since eax is written)
            // NOTE: mov to eax zero-extends to rax in x86_64
            a.mov(x86::eax, address);

            // Bounds check: address < 65536 → panic
            a.cmp(x86::rax, 65536);
            a.jb(panicLabel);

            // Runtime check: address >= memory_size → page fault
            a.cmp(x86::rax, x86::r13);  // Compare with full 64-bit memory_size
            a.jae(pagefaultLabel);

            // Load source value from register
            a.mov(x86::rdx, x86::qword_ptr(x86::rbx, srcReg * 8));

            // Store to memory [VM_MEMORY_PTR + address]
            a.mov(x86::qword_ptr(x86::r12, x86::rax), x86::rdx);

            goto next_instruction;
        }

        // For all other instructions, use the existing dispatcher
        // but only for this single instruction
        if (!jit_emitter_emit_basic_block_instructions(&a, "x86_64", codeBuffer, pc, pc + instrSize)) {
            return 3; // Compilation error
        }

    next_instruction:
        // Advance to next instruction if not already handled by jump/branch
        if (!instructionHandled) {
            pc += instrSize;
        }
    }

    // Bind panic label
    a.bind(panicLabel);
    a.mov(x86::eax, -1);  // Exit code -1 = panic(.trap)
    a.jmp(epilogueLabel);

    // Bind pagefault label
    // Note: Faulting address should be in eax (from bounds checking code)
    // Save it to VM register R0 before setting exit code
    a.bind(pagefaultLabel);
    a.mov(x86::qword_ptr(x86::rbx, 0), x86::rax);  // Save faulting address to VM R0
    a.mov(x86::eax, 3);   // Exit code 3 = pageFault
    a.jmp(epilogueLabel);

    // Bind out-of-gas label
    a.bind(outOfGasLabel);
    a.mov(x86::eax, 1);   // Exit code 1 = outOfGas
    a.jmp(epilogueLabel);

    // Bind unsupported label
    a.bind(unsupportedLabel);
    a.mov(x86::eax, 2);   // Exit code 2 = fallback to interpreter
    a.jmp(epilogueLabel);

    // Bind exit label
    a.bind(exitLabel);

    // Set return value to 0 (halt) in eax
    a.xor_(x86::eax, x86::eax);
    // Fall through to epilogue

    // Bind epilogue label (for Trap - already has eax=-1 set)
    a.bind(epilogueLabel);

    // Epilogue: restore callee-saved registers and return
    a.pop(r15);
    a.pop(r14);
    a.pop(r13);
    a.pop(r12);
    a.pop(rbp);
    a.pop(rbx);
    a.ret();

    // === DISPATCHER LOOP FOR INDIRECT JUMPS WITHOUT JUMP TABLE ===
    // This is used by JumpInd when no jump table is provided
    // It implements a computed goto using a jump table built at compile time
    // NOTE: This must come AFTER the epilogue/ret so normal execution can't reach it
    a.bind(dispatcherLoop);

    // Validate PC is within bounds
    a.cmp(x86::r15d, static_cast<uint32_t>(codeSize));
    a.jae(pagefaultLabel);  // PC out of bounds

    // Load the jump table base address from the context
    // The jump table is stored in JITHostFunctionTable.dispatcherJumpTable
    a.mov(x86::rdx, x86::qword_ptr(x86::rbp, offsetof(JITHostFunctionTable, dispatcherJumpTable)));
    a.test(x86::rdx, x86::rdx);
    a.jz(unsupportedLabel);  // No dispatcher table - fall back to interpreter

    // Jump table entry size = 8 bytes (pointer)
    // Calculate offset: PC * 8
    a.mov(x86::eax, x86::r15d);  // Load PC
    a.shl(x86::rax, 3);  // Multiply by 8 (pointer size)
    a.add(x86::rdx, x86::rax);  // Add to base

    // Load target address from jump table
    a.mov(x86::rax, x86::qword_ptr(x86::rdx));

    // Jump to the target address
    a.test(x86::rax, x86::rax);  // Check if address is null
    a.jz(unsupportedLabel);  // Null entry - fall back to interpreter
    a.jmp(x86::rax);
    // NOTE: Execution never returns here - either jumps to target or falls back

    // Generate the function code
    Error addErr = ctx->runtime->add(funcOut, &code);
    if (addErr != kErrorOk) {
        return int32_t(addErr);
    }

    // Build dispatcher jump table ONLY if needed (has JumpInd instructions)
    // This allows JumpInd to work without a PVM jump table
    if (needsDispatcherTable) {
        // OPTIMIZATION: Create dispatcher entries for ALL instruction PCs
        // This eliminates interpreter fallbacks for JumpInd to any PC
        // Previously we only created entries for jump targets, causing fallback

        // Get all PCs that have labels (now includes ALL instruction PCs when needsDispatcherTable is true)
        std::vector<uint32_t> labeledPCs = labelManager.getAllPCs();

        // Allocate table (one entry per possible PC, initialized to null)
        auto dispatcherTable = std::make_unique<void*[]>(codeSize);
        std::memset(dispatcherTable.get(), 0, codeSize * sizeof(void*));

        // Get the base address of the generated function
        uint8_t* funcBase = reinterpret_cast<uint8_t*>(*funcOut);

        // Resolve label addresses and populate the jump table
        size_t entriesPopulated = 0;
        for (uint32_t pc : labeledPCs) {
            // CRITICAL: Bounds check to prevent heap buffer overflow
            // labeledPCs may contain out-of-bounds values from markJumpTarget
            if (pc >= codeSize) {
                // Skip out-of-bounds PC values silently
                goto next_instruction;
            }

            Label label = labelManager.getLabel(pc);
            if (!label.is_valid()) {
                continue;  // Skip invalid labels
            }

            // Get the address of this label from the CodeHolder
            // CRITICAL: label_offset() can crash if label is not bound to code
            // Only call it if the label is valid and bound
            if (!labelManager.isLabelDefined(pc)) {
                goto next_instruction;
            }

            uint64_t offset = code.label_offset(label);
            dispatcherTable[pc] = funcBase + offset;
            entriesPopulated++;
        }


        // Store the dispatcher table in context for later retrieval
        // Transfer ownership to context storage
        void** tablePtr = dispatcherTable.release();
        ctx->dispatcherTables[*funcOut] = {tablePtr, static_cast<size_t>(codeSize)};
    }

    return 0; // Success
}

// ============================================================================
// MARK: - Export/Import API for Persistent Caching
// ============================================================================

/// Export compiled code metadata for serialization
/// Returns information needed to serialize compiled code
///
/// NOTE: This is a simplified version that returns basic metadata
/// Full code serialization would require deeper AsmJit integration
///
/// @param funcPtr Compiled function pointer from compilePolkaVMCode_x64_labeled
/// @param dispatcherTableOut Output: dispatcher table pointer (if exists)
/// @param dispatcherTableSizeOut Output: size of dispatcher table in entries
/// @param hasDispatcherTableOut Output: 1 if function has dispatcher table, 0 otherwise
/// @return 0 on success, error code on failure
extern "C" int32_t getCompiledCodeInfo(
    void* _Nonnull context,
    void* _Nonnull funcPtr,
    void* _Nullable * _Nullable * _Nonnull dispatcherTableOut,
    size_t* _Nonnull dispatcherTableSizeOut,
    int* _Nonnull hasDispatcherTableOut) noexcept
{
    auto* ctx = static_cast<RuntimeContext*>(context);
    if (!ctx || !funcPtr || !dispatcherTableOut || !dispatcherTableSizeOut || !hasDispatcherTableOut) {
        return -1;  // Invalid parameters
    }

    auto it = ctx->dispatcherTables.find(funcPtr);

    if (it != ctx->dispatcherTables.end()) {
        // Function has a dispatcher table
        *dispatcherTableOut = it->second.first;
        *dispatcherTableSizeOut = it->second.second;
        *hasDispatcherTableOut = 1;
    } else {
        // Function doesn't have a dispatcher table (no JumpInd)
        *dispatcherTableOut = nullptr;
        *dispatcherTableSizeOut = 0;
        *hasDispatcherTableOut = 0;
    }

    return 0;  // Success
}

/// Store compiled code metadata for a function
/// This allows tracking which bytecode corresponds to which compiled function
///
/// @param bytecodeHash Hash of the bytecode (for cache key)
/// @param funcPtr Compiled function pointer
/// @param codeSize Size of the compiled code in bytes (if known)
/// @return 0 on success, error code on failure
extern "C" int32_t setCompiledCodeMetadata(
    uint64_t bytecodeHash,
    void* _Nonnull funcPtr,
    size_t codeSize) noexcept
{
    if (!funcPtr) {
        return -1;  // Invalid parameter
    }

    // For now, this is a placeholder
    // In a full implementation, we would store this mapping in a global cache
    // This allows looking up functions by bytecode hash later

    return 0;  // Success
}

// ============================================================================
// MARK: - Memory Management (Dispatcher Table Cleanup)
// ============================================================================

/// Free the dispatcher table associated with a JIT-compiled function (x64 version)
/// This prevents memory leaks from accumulating dispatcher tables
///
/// @param context Runtime context owning the dispatcher table
/// @param funcPtr Function pointer returned by compilePolkaVMCode_x64_labeled
/// @note Safe to call with nullptr or function pointers that don't have tables
extern "C" void freeDispatcherTable_x64(
    void* _Nonnull context,
    void* _Nullable funcPtr) noexcept {
    auto* ctx = static_cast<RuntimeContext*>(context);
    if (!ctx || !funcPtr) {
        return;  // Nothing to free
    }

    auto it = ctx->dispatcherTables.find(funcPtr);

    if (it != ctx->dispatcherTables.end()) {
        // Free the dispatcher table array
        void** table = it->second.first;
        free(table);  // Use free() since it was allocated with malloc/calloc

        // Remove from map
        ctx->dispatcherTables.erase(it);
    }
}

/// Free ALL dispatcher tables (x64 version)
/// Useful for process cleanup or memory pressure situations
///
/// @param context Runtime context whose dispatcher tables should be freed
extern "C" void freeAllDispatcherTables_x64(void* _Nonnull context) noexcept {
    auto* ctx = static_cast<RuntimeContext*>(context);
    if (!ctx) {
        return;
    }

    for (auto& entry : ctx->dispatcherTables) {
        void** table = entry.second.first;
        free(table);  // Use free() since it was allocated with malloc/calloc
    }

    ctx->dispatcherTables.clear();
}

/// Release JIT-compiled code memory (x64 version)
/// This frees the machine code allocated by AsmJit's JitRuntime
///
/// @param context Runtime context owning the JIT runtime
/// @param funcPtr Function pointer to release
/// @note Safe to call with nullptr
/// @warning After calling this, the function pointer becomes invalid and must not be called
extern "C" void releaseJITFunction_x64(
    void* _Nonnull context,
    void* _Nullable funcPtr) noexcept {
    auto* ctx = static_cast<RuntimeContext*>(context);
    if (!ctx || !funcPtr) {
        return;  // Nothing to release
    }

    // Release the function memory
    ctx->runtime->release((uint8_t*)funcPtr);

    #ifdef DEBUG_JIT
    fprintf(stderr, "[JIT x64] Released JIT function %p\n", funcPtr);
    #endif
}
